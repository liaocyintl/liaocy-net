(self.webpackChunkliaocy_net=self.webpackChunkliaocy_net||[]).push([[185],{17202:function(e,n,t){"use strict";t.r(n),t.d(n,{assets:function(){return p},contentTitle:function(){return m},default:function(){return g},frontMatter:function(){return c},metadata:function(){return u},toc:function(){return d}});var a=t(87462),s=t(63366),r=(t(67294),t(3905)),l=t(93456),o=t(13066),i=["components"],c={title:"K8S Memo",authors:"liaocy",tags:["Kubernetes","k8s","k8s-tutorial"]},m=void 0,u={unversionedId:"kubernetes/k8s-memo",id:"kubernetes/k8s-memo",title:"K8S Memo",description:"This is a note of the Kubernetes tutorial.",source:"@site/docs/kubernetes/k8s-memo.mdx",sourceDirName:"kubernetes",slug:"/kubernetes/k8s-memo",permalink:"/liaocy-net/docs/kubernetes/k8s-memo",draft:!1,editUrl:"https://github.com/liaocyintl/liaocy-net/tree/main/docs/kubernetes/k8s-memo.mdx",tags:[{label:"Kubernetes",permalink:"/liaocy-net/docs/tags/kubernetes"},{label:"k8s",permalink:"/liaocy-net/docs/tags/k-8-s"},{label:"k8s-tutorial",permalink:"/liaocy-net/docs/tags/k-8-s-tutorial"}],version:"current",frontMatter:{title:"K8S Memo",authors:"liaocy",tags:["Kubernetes","k8s","k8s-tutorial"]},sidebar:"tutorialSidebar",previous:{title:"Kubernetes",permalink:"/liaocy-net/docs/category/kubernetes"}},p={},d=[{value:"References",id:"references",level:2},{value:"Physical Structure",id:"physical-structure",level:2},{value:"Single Master Node",id:"single-master-node",level:3},{value:"High-Availability Master Nodes",id:"high-availability-master-nodes",level:3},{value:"Build K8S Cluster via Kubeadm",id:"build-k8s-cluster-via-kubeadm",level:2},{value:"OS image",id:"os-image",level:3},{value:"Min Spec",id:"min-spec",level:3},{value:"Install Tools",id:"install-tools",level:3},{value:"Close Firewall",id:"close-firewall",level:3},{value:"Close swap",id:"close-swap",level:3},{value:"Set hostnames",id:"set-hostnames",level:3},{value:"Set hosts",id:"set-hosts",level:3},{value:"Write Kube Traffic in iptables",id:"write-kube-traffic-in-iptables",level:3},{value:"Install Docker",id:"install-docker",level:3},{value:"Install Kubelet",id:"install-kubelet",level:3},{value:"Init Kube Master Node",id:"init-kube-master-node",level:3},{value:"Install flannel plugin",id:"install-flannel-plugin",level:3},{value:"Install Slave Nodes",id:"install-slave-nodes",level:3},{value:"Startup Test Nginx Pod",id:"startup-test-nginx-pod",level:3},{value:"Build K8S Cluster via Binary",id:"build-k8s-cluster-via-binary",level:2},{value:"Kubectl",id:"kubectl",level:2},{value:"Delete Node",id:"delete-node",level:3},{value:"Ingress Controller",id:"ingress-controller",level:2},{value:"Create Text Pods",id:"create-text-pods",level:3},{value:"Expose Port",id:"expose-port",level:3},{value:"Create Ingress Controller",id:"create-ingress-controller",level:3},{value:"Apply Ingress Controller",id:"apply-ingress-controller",level:3},{value:"HELM",id:"helm",level:2},{value:"Introduce",id:"introduce",level:3},{value:"Usage",id:"usage",level:3},{value:"Install Helm",id:"install-helm",level:3},{value:"Config Helm Repository",id:"config-helm-repository",level:3},{value:"App Deployment",id:"app-deployment",level:3},{value:"Create Chart and Reploy",id:"create-chart-and-reploy",level:3},{value:"charts",id:"charts",level:4},{value:"Chart.yaml",id:"chartyaml",level:4},{value:"templates",id:"templates",level:4},{value:"values.yaml",id:"valuesyaml",level:4},{value:"install chart",id:"install-chart",level:4},{value:"Upgrade Chart",id:"upgrade-chart",level:4},{value:"Templates (Dynamic Parameters)",id:"templates-dynamic-parameters",level:3},{value:"NFS PV PVC",id:"nfs-pv-pvc",level:2},{value:"NFS",id:"nfs",level:3},{value:"PV and PVC",id:"pv-and-pvc",level:2},{value:"Defind PVC",id:"defind-pvc",level:3},{value:"Define PV",id:"define-pv",level:3},{value:"Test",id:"test",level:3},{value:"Monitoring",id:"monitoring",level:2},{value:"Monitoring Index",id:"monitoring-index",level:3},{value:"Prometheus",id:"prometheus",level:3},{value:"Grafana",id:"grafana",level:3},{value:"Config Grafana",id:"config-grafana",level:3},{value:"Access Grafana Web UI",id:"access-grafana-web-ui",level:4},{value:"Add data source",id:"add-data-source",level:4},{value:"Add Template",id:"add-template",level:4},{value:"See Your Monitoring Dashboard",id:"see-your-monitoring-dashboard",level:4},{value:"High Availability",id:"high-availability",level:2},{value:"Reset All Nodes",id:"reset-all-nodes",level:3},{value:"Reconfig Master Node1 (192.168.8.21)",id:"reconfig-master-node1-192168821",level:3},{value:"Install System on Master Node2 (192.168.8.21)",id:"install-system-on-master-node2-192168821",level:3},{value:"Config Keepalived on Masters",id:"config-keepalived-on-masters",level:3},{value:"Config haproxy on Masters",id:"config-haproxy-on-masters",level:3},{value:"Config K8S on Master 1",id:"config-k8s-on-master-1",level:3},{value:"Config K8S on Master 2",id:"config-k8s-on-master-2",level:3},{value:"Copy Certification Files from Master 1 to 2",id:"copy-certification-files-from-master-1-to-2",level:4},{value:"Add Master 2 into Cluster",id:"add-master-2-into-cluster",level:4},{value:"Config Slave Nodes",id:"config-slave-nodes",level:3},{value:"Test the HA Cluster",id:"test-the-ha-cluster",level:3}],k={toc:d};function g(e){var n=e.components,c=(0,s.Z)(e,i);return(0,r.kt)("wrapper",(0,a.Z)({},k,c,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"This is a note of the Kubernetes tutorial."),(0,r.kt)("h2",{id:"references"},"References"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://kubernetes.io/"},"Kubernetes")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://youtu.be/W3V-VgTjDjo"},"Kubernetes tutorial (Chinese)")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("a",{parentName:"li",href:"https://www.jianshu.com/p/db15f4994de3"},"Prometheus \u76d1\u63a7 k8s\u96c6\u7fa4\uff08\u96c6\u7fa4\u90e8\u7f72\uff09(Chinese)"))),(0,r.kt)("h2",{id:"physical-structure"},"Physical Structure"),(0,r.kt)("h3",{id:"single-master-node"},"Single Master Node"),(0,r.kt)(l.Mermaid,{config:{},chart:"graph LR\n  master[Master 192.168.8.21]\n  node1[Node1 192.168.8.31]\n  node2[Node2 192.168.8.32]\n  node3[Node2 192.168.8.33]\n  master --\x3e node1\n  master --\x3e node2\n  master --\x3e node3",mdxType:"Mermaid"}),(0,r.kt)("h3",{id:"high-availability-master-nodes"},"High-Availability Master Nodes"),(0,r.kt)("h2",{id:"build-k8s-cluster-via-kubeadm"},"Build K8S Cluster via Kubeadm"),(0,r.kt)("h3",{id:"os-image"},"OS image"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://ftp.riken.jp/Linux/centos/7.9.2009/isos/x86_64/"},"CentOS-7-x86_64-Minimal-2009.iso")),(0,r.kt)("h3",{id:"min-spec"},"Min Spec"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Lab environment (Master and Slaves)",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"CPU: 2 cores"),(0,r.kt)("li",{parentName:"ul"},"Memory: 2GB"),(0,r.kt)("li",{parentName:"ul"},"Disk: 100GB")))),(0,r.kt)("h3",{id:"install-tools"},"Install Tools"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on all nodes"',title:'"run',on:!0,all:!0,'nodes"':!0},"# Install tools\nyum install -y wget\nyum install -y nano\nyum install -y net-tools\n")),(0,r.kt)("h3",{id:"close-firewall"},"Close Firewall"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on all nodes"',title:'"run',on:!0,all:!0,'nodes"':!0},"systemctl stop firewalld\nsystemctl disable firewalld\n")),(0,r.kt)("h3",{id:"close-swap"},"Close swap"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on all nodes"',title:'"run',on:!0,all:!0,'nodes"':!0},"swapoff -a\nsed -ri 's/.*swap.*/#&/' /etc/fstab\n")),(0,r.kt)("h3",{id:"set-hostnames"},"Set hostnames"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on master"',title:'"run',on:!0,'master"':!0},"hostnamectl set-hostname kube-master\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on node1"',title:'"run',on:!0,'node1"':!0},"hostnamectl set-hostname kube-node1\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on node2"',title:'"run',on:!0,'node2"':!0},"hostnamectl set-hostname kube-node2\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on node3"',title:'"run',on:!0,'node3"':!0},"hostnamectl set-hostname kube-node3\n")),(0,r.kt)("h3",{id:"set-hosts"},"Set hosts"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on master"',title:'"run',on:!0,'master"':!0},"nano /etc/hosts\n")),(0,r.kt)("p",null,"Fill in the following contents:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text",metastring:'title="/etc/hosts"',title:'"/etc/hosts"'},"192.168.8.21 kube-master\n192.168.8.31 kube-node1\n192.168.8.32 kube-node2\n192.168.8.33 kube-node3\n")),(0,r.kt)("h3",{id:"write-kube-traffic-in-iptables"},"Write Kube Traffic in iptables"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on all nodes"',title:'"run',on:!0,all:!0,'nodes"':!0},"cat <<EOF > /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n\nsysctl --system\n")),(0,r.kt)("h3",{id:"install-docker"},"Install Docker"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on all nodes"',title:'"run',on:!0,all:!0,'nodes"':!0},"yum install -y yum-utils\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum install -y docker-ce-18.06.1.ce-3.el7 docker-ce-cli-18.06.1.ce-3.el7 containerd.io docker-compose-plugin\nsystemctl enable docker && systemctl start docker\ndocker --version\n")),(0,r.kt)("h3",{id:"install-kubelet"},"Install Kubelet"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on all nodes"',title:'"run',on:!0,all:!0,'nodes"':!0},"cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\n\nsetenforce 0\nsed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n\nyum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\nsystemctl enable kubelet\n\n")),(0,r.kt)("h3",{id:"init-kube-master-node"},"Init Kube Master Node"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on master"',title:'"run',on:!0,'master"':!0},"kubeadm init --apiserver-advertise-address=192.168.8.21 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16\n\nmkdir -p $HOME/.kube\ncp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nchown $(id -u):$(id -g) $HOME/.kube/config\nkubectl get nodes\n")),(0,r.kt)("p",null,"The token is only valid for 24-hours. You can use the following command to generate a new one."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubeadm token create --print-join-command\n")),(0,r.kt)("h3",{id:"install-flannel-plugin"},"Install flannel plugin"),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `kube-flannel.yml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},'---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default\nspec:\n  privileged: false\n  volumes:\n  - configMap\n  - secret\n  - emptyDir\n  - hostPath\n  allowedHostPaths:\n  - pathPrefix: "/etc/cni/net.d"\n  - pathPrefix: "/etc/kube-flannel"\n  - pathPrefix: "/run/flannel"\n  readOnlyRootFilesystem: false\n  # Users and groups\n  runAsUser:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  # Privilege Escalation\n  allowPrivilegeEscalation: false\n  defaultAllowPrivilegeEscalation: false\n  # Capabilities\n  allowedCapabilities: [\'NET_ADMIN\', \'NET_RAW\']\n  defaultAddCapabilities: []\n  requiredDropCapabilities: []\n  # Host namespaces\n  hostPID: false\n  hostIPC: false\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  # SELinux\n  seLinux:\n    # SELinux is unused in CaaSP\n    rule: \'RunAsAny\'\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n- apiGroups: [\'extensions\']\n  resources: [\'podsecuritypolicies\']\n  verbs: [\'use\']\n  resourceNames: [\'psp.flannel.unprivileged\']\n- apiGroups:\n  - ""\n  resources:\n  - pods\n  verbs:\n  - get\n- apiGroups:\n  - ""\n  resources:\n  - nodes\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - ""\n  resources:\n  - nodes/status\n  verbs:\n  - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      "name": "cbr0",\n      "cniVersion": "0.3.1",\n      "plugins": [\n        {\n          "type": "flannel",\n          "delegate": {\n            "hairpinMode": true,\n            "isDefaultGateway": true\n          }\n        },\n        {\n          "type": "portmap",\n          "capabilities": {\n            "portMappings": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      "Network": "10.244.0.0/16",\n      "Backend": {\n        "Type": "vxlan"\n      }\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/os\n                operator: In\n                values:\n                - linux\n      hostNetwork: true\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni-plugin\n       #image: flannelcni/flannel-cni-plugin:v1.1.0 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel-cni-plugin:v1.0.1\n        command:\n        - cp\n        args:\n        - -f\n        - /flannel\n        - /opt/cni/bin/flannel\n        volumeMounts:\n        - name: cni-plugin\n          mountPath: /opt/cni/bin\n      - name: install-cni\n       #image: flannelcni/flannel:v0.18.0 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel:v0.16.3\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n       #image: flannelcni/flannel:v0.18.0 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel:v0.16.3\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: "100m"\n            memory: "50Mi"\n          limits:\n            cpu: "100m"\n            memory: "50Mi"\n        securityContext:\n          privileged: false\n          capabilities:\n            add: ["NET_ADMIN", "NET_RAW"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: EVENT_QUEUE_DEPTH\n          value: "5000"\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n      volumes:\n      - name: run\n        hostPath:\n          path: /run/flannel\n      - name: cni-plugin\n        hostPath:\n          path: /opt/cni/bin\n      - name: cni\n        hostPath:\n          path: /etc/cni/net.d\n      - name: flannel-cfg\n        configMap:\n          name: kube-flannel-cfg\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on master"',title:'"run',on:!0,'master"':!0},"$ kubectl apply -f kube-flannel.yml\n$ kubectl get pods -n kube-system\n$ kubectl get nodes\n")),(0,r.kt)("div",{className:"admonition admonition-warning alert alert--danger"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Installing flannel plugin may cause some problems.")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Referencing ",(0,r.kt)("a",{parentName:"p",href:"https://none06.hatenadiary.org/entry/2022/05/28/025115"},"https://none06.hatenadiary.org/entry/2022/05/28/025115")),(0,r.kt)("pre",{parentName:"div"},(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="Official Kube Flannel Yaml"',title:'"Official',Kube:!0,Flannel:!0,'Yaml"':!0},"wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\nnano kube-flannel.yml\n")),(0,r.kt)("p",{parentName:"div"},(0,r.kt)("img",{alt:"alt",src:t(26906).Z,title:"title",width:"706",height:"500"})))),(0,r.kt)("h3",{id:"install-slave-nodes"},"Install Slave Nodes"),(0,r.kt)("p",null,"This command should be copied from the contents that the master node generated."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on slave nodes"',title:'"run',on:!0,slave:!0,'nodes"':!0}," kubeadm join 192.168.8.21:6443 --token fxm6sy.jsw1uyarkx2fzsnw \\\n    --discovery-token-ca-cert-hash sha256:77c6f969d3e43942e4dea3b3c413ac9178fe89a2a6ac1a98254593eba574719e\n")),(0,r.kt)("h3",{id:"startup-test-nginx-pod"},"Startup Test Nginx Pod"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on master"',title:'"run',on:!0,'master"':!0},"kubectl create deployment nginx --image=nginx\nkubectl get pods\nkubectl expose deployment nginx --port=80 --type=NodePort\nkubectl get pod, svc\nkubectl get pods -A -o wide\n")),(0,r.kt)("p",null,"Then you can access the Nginx service via the URL from any node."),(0,r.kt)("p",null,"http://<node_ip>:<exposed_port>/"),(0,r.kt)("h2",{id:"build-k8s-cluster-via-binary"},"Build K8S Cluster via Binary"),(0,r.kt)("p",null,"pending"),(0,r.kt)("h2",{id:"kubectl"},"Kubectl"),(0,r.kt)("h3",{id:"delete-node"},"Delete Node"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on master"',title:'"run',on:!0,'master"':!0},"kubectl get nodes\n# NAME          STATUS   ROLES    AGE   VERSION\n# kube-master   Ready    master   10d   v1.18.0\n# kube-node1    Ready    <none>   10d   v1.18.0\n# kube-node2    Ready    <none>   10d   v1.18.0\n# kube-node3    Ready    <none>   10d   v1.18.0\n\nkubectl delete node kube-node3\n# NAME          STATUS   ROLES    AGE   VERSION\n# kube-master   Ready    master   10d   v1.18.0\n# kube-node1    Ready    <none>   10d   v1.18.0\n# kube-node2    Ready    <none>   10d   v1.18.0\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on kube-node3"',title:'"run',on:!0,'kube-node3"':!0},"kubeadm reset\n# [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.\n# [reset] Are you sure you want to proceed? [y/N]: y\n# [preflight] Running pre-flight checks\n# W0613 05:24:48.614124   25596 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory\n# [reset] No etcd config found. Assuming external etcd\n# [reset] Please, manually reset etcd to prevent further issues\n# [reset] Stopping the kubelet service\n# [reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n# [reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]\n# [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n# [reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]\n\n# The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n\n# The reset process does not reset or clean up iptables rules or IPVS tables.\n# If you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n\n# If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\n# to reset your system's IPVS tables.\n\n# The reset process does not clean your kubeconfig files and you must remove them manually.\n# Please, check the contents of the $HOME/.kube/config file.\n")),(0,r.kt)("h2",{id:"ingress-controller"},"Ingress Controller"),(0,r.kt)("p",null,"Ingress Controller is a Kubernetes add-on that provides a simple way to expose your services to the internet."),(0,r.kt)("h3",{id:"create-text-pods"},"Create Text Pods"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create deployment web --image=nginx\nkubectl get pods -o wide\n\n# NAME                   READY   STATUS              RESTARTS   AGE   IP       NODE         NOMINATED NODE   READINESS GATES\n# web-5dcb957ccc-px72n   0/1     ContainerCreating   0          7s    <none>   kube-node1   <none>           <none>\n\n")),(0,r.kt)("h3",{id:"expose-port"},"Expose Port"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"\nkubectl expose deployment web --port=80 --target-port=80 --type=NodePort\n\nkubectl get svc\n\n# NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n# kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        3d14h\n# web          NodePort    10.109.213.80   <none>        80:30622/TCP   70s\n\n")),(0,r.kt)("h3",{id:"create-ingress-controller"},"Create Ingress Controller"),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `ingress-controller.yml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},'apiVersion: v1\nkind: Namespace\nmetadata:\n  name: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: nginx-configuration\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: udp-services\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ingress-serviceaccount\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: nginx-ingress-clusterrole\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\nrules:\n  - apiGroups:\n      - ""\n    resources:\n      - configmaps\n      - endpoints\n      - nodes\n      - pods\n      - secrets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - ""\n    resources:\n      - nodes\n    verbs:\n      - get\n  - apiGroups:\n      - ""\n    resources:\n      - services\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - "extensions"\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - ""\n    resources:\n      - events\n    verbs:\n      - create\n      - patch\n  - apiGroups:\n      - "extensions"\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: nginx-ingress-role\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\nrules:\n  - apiGroups:\n      - ""\n    resources:\n      - configmaps\n      - pods\n      - secrets\n      - namespaces\n    verbs:\n      - get\n  - apiGroups:\n      - ""\n    resources:\n      - configmaps\n    resourceNames:\n      # Defaults to "<election-id>-<ingress-class>"\n      # Here: "<ingress-controller-leader>-<nginx>"\n      # This has to be adapted if you change either parameter\n      # when launching the nginx-ingress-controller.\n      - "ingress-controller-leader-nginx"\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - ""\n    resources:\n      - configmaps\n    verbs:\n      - create\n  - apiGroups:\n      - ""\n    resources:\n      - endpoints\n    verbs:\n      - get\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: nginx-ingress-role-nisa-binding\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: nginx-ingress-role\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: nginx-ingress-clusterrole-nisa-binding\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: nginx-ingress-clusterrole\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/part-of: ingress-nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/part-of: ingress-nginx\n      annotations:\n        prometheus.io/port: "10254"\n        prometheus.io/scrape: "true"\n    spec:\n      hostNetwork: true\n      serviceAccountName: nginx-ingress-serviceaccount\n      containers:\n        - name: nginx-ingress-controller\n          image: siriuszg/nginx-ingress-controller:0.20.0\n          args:\n            - /nginx-ingress-controller\n            - --configmap=$(POD_NAMESPACE)/nginx-configuration\n            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n            - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n            - --annotations-prefix=nginx.ingress.kubernetes.io\n          securityContext:\n            allowPrivilegeEscalation: true\n            capabilities:\n              drop:\n                - ALL\n              add:\n                - NET_BIND_SERVICE\n            # www-data -> 33\n            runAsUser: 33\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          ports:\n            - name: http\n              containerPort: 80\n            - name: https\n              containerPort: 443\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx\n  namespace: ingress-nginx\nspec:\n  #type: NodePort\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n      protocol: TCP\n    - name: https\n      port: 443\n      targetPort: 443\n      protocol: TCP\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="run on master"',title:'"run',on:!0,'master"':!0},"kubectl apply -f ingress-controller.yml \n\n# namespace/ingress-nginx created\n# configmap/nginx-configuration created\n# configmap/tcp-services created\n# configmap/udp-services created\n# serviceaccount/nginx-ingress-serviceaccount created\n# clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created\n# role.rbac.authorization.k8s.io/nginx-ingress-role created\n# rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created\n# clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created\n# daemonset.apps/nginx-ingress-controller created\n# service/ingress-nginx created\n\n")),(0,r.kt)("h3",{id:"apply-ingress-controller"},"Apply Ingress Controller"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods -n ingress-nginx\n\n# NAME                             READY   STATUS              RESTARTS   AGE\n# nginx-ingress-controller-57wqf   0/1     ContainerCreating   0          111s\n# nginx-ingress-controller-5spw2   0/1     Running             0          111s\n# nginx-ingress-controller-kk552   0/1     ContainerCreating   0          111s\n")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `ingress.yml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n  - host: example.ingredemo.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName:\n          servicePort: 80")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f ingress.yaml \n# ingress.networking.k8s.io/example-ingress created\n\nkubectl get ing\n# NAME              CLASS    HOSTS                   ADDRESS   PORTS   AGE\n# example-ingress   <none>   example.ingredemo.com             80      5m52s\n")),(0,r.kt)("p",null,"Then you can access the service with the ",(0,r.kt)("inlineCode",{parentName:"p"},"example.ingredemo.com"),"."),(0,r.kt)("div",{className:"admonition admonition-warning alert alert--danger"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"warning")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},(0,r.kt)("inlineCode",{parentName:"p"},"example.ingredemo.com")," should be specified as the IP of any slave node."))),(0,r.kt)("h2",{id:"helm"},"HELM"),(0,r.kt)("h3",{id:"introduce"},"Introduce"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"helm: a package manager for Kubernetes")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Chart: a collection of Kubernetes resources")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Release: version controll"))),(0,r.kt)("h3",{id:"usage"},"Usage"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Manage YAML"),(0,r.kt)("li",{parentName:"ul"},"Reuse YAML"),(0,r.kt)("li",{parentName:"ul"},"Version Control")),(0,r.kt)("h3",{id:"install-helm"},"Install Helm"),(0,r.kt)("p",null,"Ref: ",(0,r.kt)("a",{parentName:"p",href:"https://helm.sh/docs/intro/quickstart/"},"https://helm.sh/docs/intro/quickstart/")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"wget https://get.helm.sh/helm-v3.9.0-linux-amd64.tar.gz\ntar zxvf helm-v3.9.0-linux-amd64.tar.gz\ncp linux-amd64/helm /usr/bin/\nhelm version\n")),(0,r.kt)("h3",{id:"config-helm-repository"},"Config Helm Repository"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"helm repo add stable https://charts.helm.sh/stable\nhelm repo add alicloud https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts\nhelm repo list\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="update repo"',title:'"update','repo"':!0},"helm repo update\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="remove repo"',title:'"remove','repo"':!0},"helm repo remove alicloud\n")),(0,r.kt)("h3",{id:"app-deployment"},"App Deployment"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"helm search repo weave\n# NAME                    CHART VERSION   APP VERSION     DESCRIPTION                                       \n# stable/weave-cloud      0.3.9           1.4.0           DEPRECATED - Weave Cloud is a add-on to Kuberne...\n# stable/weave-scope      1.1.12          1.12.0          DEPRECATED - A Helm chart for the Weave Scope c...\nhelm install ui stable/weave-scope\nhelm list\nhelm status ui\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="Check Service and Expose Port"',title:'"Check',Service:!0,and:!0,Expose:!0,'Port"':!0},"kubectl get svc\n# NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n# ui-weave-scope   ClusterIP   10.97.190.230   <none>        80/TCP         3m31s\nkubectl edit svc ui-weave-scope\n# Change 'type: ClusterIP' into 'type: NodePort'\nkubectl get svc\nNAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n# ui-weave-scope   NodePort    10.97.190.230   <none>        80:32320/TCP   5m55s\n")),(0,r.kt)("h3",{id:"create-chart-and-reploy"},"Create Chart and Reploy"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"helm create mychart\n# Creating mychart\nls mychart/\n# charts  Chart.yaml  templates  values.yaml\ncd mychart/\n")),(0,r.kt)("h4",{id:"charts"},"charts"),(0,r.kt)("h4",{id:"chartyaml"},"Chart.yaml"),(0,r.kt)("p",null,"Current chart attributes."),(0,r.kt)("h4",{id:"templates"},"templates"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd templates/\nrm -rf *\n\nkubectl create deployment web1 --image=nginx --dry-run=client -o yaml > deployment.yaml\n\nkubectl create deployment web1 --image=nginx\nkubectl expose deployment web1 --port=80 --target-port=80 --type=NodePort --dry-run=client -o yaml > service.yaml\nkubectl delete deployment web1\n")),(0,r.kt)("h4",{id:"valuesyaml"},"values.yaml"),(0,r.kt)("p",null,"Some global variables are used in the templates."),(0,r.kt)("h4",{id:"install-chart"},"install chart"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd /root/\nhelm install web1 mychart/\n# NAME: web1\n# LAST DEPLOYED: Wed Jun  8 06:51:05 2022\n# NAMESPACE: default\n# STATUS: deployed\n# REVISION: 1\n# TEST SUITE: None\n\nkubectl get pods -o wide\n# NAME                                            READY   STATUS    RESTARTS   AGE     IP             NODE          NOMINATED NODE   READINESS GATES\n# web1-7f87dfbd56-pz6wh                           1/1     Running   0          31s     10.244.4.72    kube-node3    <none>           <none>\n\nkubectl get svc\n# NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n# web1             NodePort    10.97.201.222   <none>        80:31459/TCP   76s\n")),(0,r.kt)("h4",{id:"upgrade-chart"},"Upgrade Chart"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'helm upgrade web1 mychart/\n# Release "web1" has been upgraded. Happy Helming!\n# NAME: web1\n# LAST DEPLOYED: Wed Jun  8 06:54:14 2022\n# NAMESPACE: default\n# STATUS: deployed\n# REVISION: 2\n# TEST SUITE: None\n')),(0,r.kt)("h3",{id:"templates-dynamic-parameters"},"Templates (Dynamic Parameters)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd /root/mychart/\n")),(0,r.kt)("p",null,"Change ",(0,r.kt)("inlineCode",{parentName:"p"},"values.yaml")," as following:"),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `values.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},'# Default values for mychart.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nreplicaCount: 1\n\nimage: nginx\ntag: 1.1\nlabel: nginx\nport: 80\n\nimagePullSecrets: []\nnameOverride: ""\nfullnameOverride: ""\n\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: ""\n\npodAnnotations: {}\n\npodSecurityContext: {}\n  # fsGroup: 2000\n\nsecurityContext: {}\n  # capabilities:\n  #   drop:\n  #   - ALL\n  # readOnlyRootFilesystem: true\n  # runAsNonRoot: true\n  # runAsUser: 1000\n\nservice:\n  type: ClusterIP\n  port: 80\n\ningress:\n  enabled: false\n  className: ""\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: "true"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nresources: {}\n  # We usually recommend not to specify default resources and to leave this as a conscious\n  # choice for the user. This also increases chances charts run on environments with little\n  # resources, such as Minikube. If you do want to specify resources, uncomment the following\n  # lines, adjust them as necessary, and remove the curly braces after \'resources:\'.\n  # limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd mychart/templates/\n\n")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `deployment.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: {{ .Release.Name}}\n  name: {{ .Release.Name}}-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: {{ .Values.label}}\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: {{ .Values.label}}   \n    spec:\n      containers:\n      - image: {{ .Values.image}}\n        name: nginx\n        resources: {}\nstatus: {}")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'cd /root\nhelm install --dry-run web2 mychart/\nhelm install web2 mychart/\n# Release "web2" has been upgraded. Happy Helming!\n# NAME: web2\n# LAST DEPLOYED: Wed Jun  8 07:15:51 2022\n# NAMESPACE: default\n# STATUS: deployed\n# REVISION: 1\n# TEST SUITE: None\n\nkubectl get pods\n# NAME                                            READY   STATUS    RESTARTS   AGE\n# web2-deploy-f89759699-9lkgw                     1/1     Running   0          59s\n\nkubectl get svc\n# NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n# web2-svc         NodePort    10.97.103.186   <none>        80:31374/TCP   44s\n')),(0,r.kt)("h2",{id:"nfs-pv-pvc"},"NFS PV PVC"),(0,r.kt)("h3",{id:"nfs"},"NFS"),(0,r.kt)(l.Mermaid,{chart:"graph LR\n  master[Master 192.168.8.21]\n  node1[Node1 192.168.8.31]\n  node2[Node2 192.168.8.32]\n  node3[Node2 192.168.8.33]\n  nfs[NSF Server 192.168.8.10]\n  master --\x3e node1\n  master --\x3e node2\n  master --\x3e node3\n  nfs --\x3e node1\n  nfs --\x3e node2\n  nfs --\x3e node3",mdxType:"Mermaid"}),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on NFS Server"',title:'"on',NFS:!0,'Server"':!0},'yum install -y nfs-utils\nmkdir -p /data/nfs\necho "/data/nfs *(rw,sync,no_root_squash,no_subtree_check)" >> /etc/exports\nsystemctl start rpcbind\nsystemctl start nfs\nsystemctl enable rpcbind\nsystemctl enable nfs\nps -ef | grep nfs\n\necho "<h1>hello nfs</h1>" > /data/nfs/index.html\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on node1 and node2 and node3"',title:'"on',node1:!0,and:!0,node2:!0,'node3"':!0},"yum install -y nfs-utils\n")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `nfs-nginx.yaml` on master"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nfs-dep1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: wwwroot\n          mountPath: /usr/share/nginx/html\n        ports:\n        - containerPort: 80\n      volumes:\n      - name: wwwroot\n        nfs:\n          server: 192.168.8.10\n          path: /data/nfs\n          ")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl apply -f nfs-nginx.yaml\nkubectl get pods\n# NAME                                            READY   STATUS              RESTARTS   AGE\n# nfs-dep1-6b58cbd59-f7g4s                        0/1     ContainerCreating   0          53s\nkubectl exec -it nfs-dep1-6b58cbd59-f7g4s -- /bin/bash\nls /usr/share/nginx/html\n# index.html\nkubectl expose deployment nginx-dep1 --port=80 --target-port=80 --type=NodePort\n")),(0,r.kt)("h2",{id:"pv-and-pvc"},"PV and PVC"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"PV: Permanent Volume, A provider volume that abstracts storage resources.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"PVC: Persistent Volume Claim, A consumer requests a volume.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Procedure:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Develop app"),(0,r.kt)("li",{parentName:"ul"},"Defind PVC"),(0,r.kt)("li",{parentName:"ul"},"Defind PV")))),(0,r.kt)("h3",{id:"defind-pvc"},"Defind PVC"),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `pvc.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-dep1\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: wwwroot\n          mountPath: /usr/share/nginx/html\n        ports:\n        - containerPort: 80\n      volumes:\n      - name: wwwroot\n        persistentVolumeClaim:\n          claimName: my-pvc\n          \n---\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl apply -f pvc.yaml\n")),(0,r.kt)("h3",{id:"define-pv"},"Define PV"),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `pv.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  nfs:\n    server: 192.168.8.10\n    path: /data/nfs")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl apply -f pv.yaml\n# persistentvolume/my-pv created\n\nkubectl get pv,pvc\n# NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE\n# persistentvolume/my-pv   5Gi        RWX            Retain           Bound    default/my-pvc                           52s\n# NAME                           STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n# persistentvolumeclaim/my-pvc   Bound    my-pv    5Gi        RWX                           4m7s\n\nkubectl get pods\n# NAME                          READY   STATUS        RESTARTS   AGE\n# nginx-dep1-58b7bf955f-btpv4   1/1     Running       0          4m50s\n# nginx-dep1-58b7bf955f-t97hw   1/1     Running       0          4m50s\n# nginx-dep1-58b7bf955f-vzj8x   1/1     Running       0          4m50s\n")),(0,r.kt)("h3",{id:"test"},"Test"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl exec -it nginx-dep1-58b7bf955f-btpv4 -- /bin/bash\ncat /usr/share/nginx/html/index.html \n# <h1>hello nfs</h1>\n")),(0,r.kt)("h2",{id:"monitoring"},"Monitoring"),(0,r.kt)("h3",{id:"monitoring-index"},"Monitoring Index"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Cluster Monitoring",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Node Resouce Utillzation"),(0,r.kt)("li",{parentName:"ul"},"Node Number"),(0,r.kt)("li",{parentName:"ul"},"Node Pods"))),(0,r.kt)("li",{parentName:"ul"},"Pods Monitoring",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Containers "),(0,r.kt)("li",{parentName:"ul"},"Applications")))),(0,r.kt)("h3",{id:"prometheus"},"Prometheus"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"An open-source monitoring system."),(0,r.kt)("li",{parentName:"ul"},"Monitoring, Alerting, and DataStorage."),(0,r.kt)("li",{parentName:"ul"},"Using HTTP to extract K8S components status periodically.")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `rbac-setup.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},'apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [""]\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs: ["get", "list", "watch"]\n- apiGroups:\n  - extensions\n  resources:\n  - ingresses\n  verbs: ["get", "list", "watch"]\n- nonResourceURLs: ["/metrics"]\n  verbs: ["get"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: kube-system\n')),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `configmap.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\n  namespace: kube-system\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval:     15s\n      evaluation_interval: 15s\n    scrape_configs:\n    - job_name: 'kubernetes-apiservers'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n    - job_name: 'kubernetes-nodes'\n      kubernetes_sd_configs:\n      - role: node\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/${1}/proxy/metrics\n    - job_name: 'kubernetes-cadvisor'\n      kubernetes_sd_configs:\n      - role: node\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor\n    - job_name: 'kubernetes-service-endpoints'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n        action: replace\n        target_label: __scheme__\n        regex: (https?)\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n        action: replace\n        target_label: __address__\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        action: replace\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-services'\n      kubernetes_sd_configs:\n      - role: service\n      metrics_path: /probe\n      params:\n        module: [http_2xx]\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__address__]\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-ingresses'\n      kubernetes_sd_configs:\n      - role: ingress\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]\n        regex: (.+);(.+);(.+)\n        replacement: ${1}://${2}${3}\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_ingress_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_ingress_name]\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `prometheus.deploy.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: prometheus-deployment\n  name: prometheus\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n    spec:\n      containers:\n      - image: prom/prometheus:v2.0.0\n        name: prometheus\n        command:\n        - "/bin/prometheus"\n        args:\n        - "--config.file=/etc/prometheus/prometheus.yml"\n        - "--storage.tsdb.path=/prometheus"\n        - "--storage.tsdb.retention=24h"\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        volumeMounts:\n        - mountPath: "/prometheus"\n          name: data\n        - mountPath: "/etc/prometheus"\n          name: config-volume\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 2500Mi\n      serviceAccountName: prometheus    \n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: config-volume\n        configMap:\n          name: prometheus-config   ')),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `prometheus.svc.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n  - port: 9090\n    targetPort: 9090\n    nodePort: 30003\n  selector:\n    app: prometheus")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `node-exporter.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  namespace: kube-system\n  labels:\n    k8s-app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      k8s-app: node-exporter\n  template:\n    metadata:\n      labels:\n        k8s-app: node-exporter\n    spec:\n      containers:\n      - image: prom/node-exporter\n        name: node-exporter\n        ports:\n        - containerPort: 9100\n          protocol: TCP\n          name: http\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: node-exporter\n  name: node-exporter\n  namespace: kube-system\nspec:\n  ports:\n  - name: http\n    port: 9100\n    nodePort: 31672\n    protocol: TCP\n  type: NodePort\n  selector:\n    k8s-app: node-exporter")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"\nkubectl create -f node-exporter.yaml\n# daemonset.apps/node-exporter created\n# service/node-exporter created\n\nkubectl create -f rbac-setup.yaml\n# clusterrolebinding.rbac.authorization.k8s.io/prometheus created\n\nkubectl create -f configmap.yaml\n# configmap/prometheus-config created\n\nkubectl create -f prometheus.deploy.yaml \n# deployment.apps/prometheus created\n\nkubectl create -f prometheus.svc.yaml \n# service/prometheus created\n\nkubectl get pods -n kube-system\n# NAME                                  READY   STATUS              RESTARTS   AGE\n# node-exporter-4qlmd                   1/1     Running   0          5m36s\n# node-exporter-bhfhg                   1/1     Running   0          5m35s\n# prometheus-7486bf7f4b-mdkcm           1/1     Running   0          95s\n")),(0,r.kt)("h3",{id:"grafana"},"Grafana"),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `grafana-deploy.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-core\n  namespace: kube-system\n  labels:\n    app: grafana\n    component: core\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n        component: core\n    spec:\n      containers:\n      - image: grafana/grafana:4.2.0\n        name: grafana-core\n        imagePullPolicy: IfNotPresent\n        # env:\n        resources:\n          # keep request = limit to keep this container in guaranteed class\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n          # The following env variables set up basic auth twith the default admin user and admin password.\n          - name: GF_AUTH_BASIC_ENABLED\n            value: "true"\n          - name: GF_AUTH_ANONYMOUS_ENABLED\n            value: "false"\n          # - name: GF_AUTH_ANONYMOUS_ORG_ROLE\n          #   value: Admin\n          # does not really work, because of template variables in exported dashboards:\n          # - name: GF_DASHBOARDS_JSON_ENABLED\n          #   value: "true"\n        readinessProbe:\n          httpGet:\n            path: /login\n            port: 3000\n          # initialDelaySeconds: 30\n          # timeoutSeconds: 1\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var\n      volumes:\n      - name: grafana-persistent-storage\n        emptyDir: {}')),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `grafana-svc.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: kube-system\n  labels:\n    app: grafana\n    component: core\nspec:\n  type: NodePort\n  ports:\n    - port: 3000\n  selector:\n    app: grafana\n    component: core")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Save as `grafana-ing.yaml`"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n   name: grafana\n   namespace: kube-system\nspec:\n   rules:\n   - host: k8s.grafana\n     http:\n       paths:\n       - path: /\n         backend:\n          serviceName: grafana\n          servicePort: 3000")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl create -f grafana-deploy.yaml\n# deployment.apps/grafana-core created\n\nkubectl create -f grafana-svc.yaml\n# service/grafana created\n\nkubectl create -f grafana-ing.yaml\n# ingress.extensions/grafana created\n\nkubectl get pods -n kube-system\n# NAME                                  READY   STATUS    RESTARTS   AGE\n# grafana-core-768b6bf79c-xdgj4         0/1     Running   0          117s\n# prometheus-7486bf7f4b-mdkcm           1/1     Running   0          9m\n")),(0,r.kt)("h3",{id:"config-grafana"},"Config Grafana"),(0,r.kt)("h4",{id:"access-grafana-web-ui"},"Access Grafana Web UI"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl get svc -n kube-system\n# NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE\n# grafana         NodePort    10.96.26.29      <none>        3000:30178/TCP           2m48s\n# prometheus      NodePort    10.108.160.161   <none>        9090:30003/TCP           10m\n")),(0,r.kt)("p",null,"Therefore, access Grafana ",(0,r.kt)("a",{parentName:"p",href:"http://192.168.8.31:30178/login"},"http://192.168.8.31:30178/login")," on your browser."),(0,r.kt)("p",null,"Default user is ",(0,r.kt)("inlineCode",{parentName:"p"},"admin")," and password is ",(0,r.kt)("inlineCode",{parentName:"p"},"admin"),"."),(0,r.kt)("h4",{id:"add-data-source"},"Add data source"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt",src:t(53066).Z,title:"title",width:"771",height:"485"})),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt",src:t(58803).Z,title:"title",width:"1097",height:"396"})),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt",src:t(95007).Z,title:"title",width:"1049",height:"695"})),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"Url")," should be the ",(0,r.kt)("inlineCode",{parentName:"p"},"CLUSTER-IP")," which you can get from ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl get svc -n kube-system"),"."),(0,r.kt)("h4",{id:"add-template"},"Add Template"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt",src:t(32711).Z,title:"title",width:"680",height:"465"})),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt",src:t(382).Z,title:"title",width:"792",height:"543"})),(0,r.kt)("p",null,"Notice that, the 315 is a template of K8S Monitoring."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt",src:t(99194).Z,title:"title",width:"814",height:"468"})),(0,r.kt)("h4",{id:"see-your-monitoring-dashboard"},"See Your Monitoring Dashboard"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"alt",src:t(81732).Z,title:"title",width:"1298",height:"716"})),(0,r.kt)("h2",{id:"high-availability"},"High Availability"),(0,r.kt)("p",null,"The current cluster architecture is dangerous if the single master node is down.\nIt should be constituted as a HA cluster as follows."),(0,r.kt)(l.Mermaid,{chart:"graph LR\n  master1[Master1 192.168.8.21]\n  master2[Master2 192.168.8.22]\n  lb[Load Balancer (VIP 192.168.8.20)]\n  node1[Node1 192.168.8.31]\n  node2[Node2 192.168.8.32]\n  master1 --\x3e lb\n  master2 --\x3e lb\n  lb --\x3e node1\n  lb --\x3e node2",mdxType:"Mermaid"}),(0,r.kt)("h3",{id:"reset-all-nodes"},"Reset All Nodes"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},'kubectl delete node kube-node1\n# node "kube-node1" deleted\nkubectl delete node kube-node2\n# node "kube-node2" deleted\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on slave nodes"',title:'"on',slave:!0,'nodes"':!0},"kubeadm reset\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubeadm reset\n")),(0,r.kt)("h3",{id:"reconfig-master-node1-192168821"},"Reconfig Master Node1 (192.168.8.21)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text",metastring:'title="on master, replace /etc/hosts"',title:'"on',"master,":!0,replace:!0,'/etc/hosts"':!0},"127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.8.20 kube-master.k8s.io kube-master\n192.168.8.21 kube-master1.k8s.io kube-master1\n192.168.8.22 kube-master2.k8s.io kube-master2\n192.168.8.31 kube-node1.k8s.io kube-node1\n192.168.8.32 kube-node2.k8s.io kube-node2\n192.168.8.33 kube-node3.k8s.io kube-node3\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1"',title:'"on','master1"':!0},"hostnamectl set-hostname kube-master1\n")),(0,r.kt)("h3",{id:"install-system-on-master-node2-192168821"},"Install System on Master Node2 (192.168.8.21)"),(0,r.kt)("p",null,"Do init installation on master node 2 following ",(0,r.kt)("inlineCode",{parentName:"p"},"Build K8S Cluster via Kubeadm")," section."),(0,r.kt)("p",null,"Notice that the ",(0,r.kt)("inlineCode",{parentName:"p"},"hosts")," file on master node 2 should be the same as master node 1."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text",metastring:'title="on master2 etc/hosts"',title:'"on',master2:!0,'etc/hosts"':!0},"127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.8.20 kube-master.k8s.io kube-master\n192.168.8.21 kube-master1.k8s.io kube-master1\n192.168.8.22 kube-master2.k8s.io kube-master2\n192.168.8.31 kube-node1.k8s.io kube-node1\n192.168.8.32 kube-node2.k8s.io kube-node2\n192.168.8.33 kube-node3.k8s.io kube-node3\n")),(0,r.kt)("h3",{id:"config-keepalived-on-masters"},"Config Keepalived on Masters"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1 and master2"',title:'"on',master1:!0,and:!0,'master2"':!0},"yum install -y keepalived conntrack-tools libseccomp libtool-ltd1\n")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Config `/etc/keepalived/keepalived.conf` referencing follows"),(0,r.kt)(o.Z,{language:"conf",mdxType:"CodeBlock"},'! Configuration File for keepalived\n\nglobal_defs {\n  router_id k8s\n}\n\nvrrp_script check_haproxy {\n  script "killall -0 haproxy"\n  interval 3\n  weight -2\n  fall 10\n  rise 2\n}\n\nvrrp_instance VI_1 {\n  state MASTER\n  interface eth0\n  virtual_router_id 51\n  priority 250\n  advert_int 1\n  authentication {\n    auth_type PASS\n    auth_pass 1111\n  }\n  virtual_ipaddress {\n    192.168.8.20\n  }\n  track_script {\n    check_haproxy\n  }\n}')),(0,r.kt)("p",null,"Notice that the ",(0,r.kt)("inlineCode",{parentName:"p"},"interface")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"virtual_ipaddress")," should be modified according to your environment."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1 and master2"',title:'"on',master1:!0,and:!0,'master2"':!0},"systemctl start keepalived.service\nsystemctl enable keepalived.service\nsystemctl status keepalived.service\n# ...\n# Jun 13 06:21:04 kube-master1 Keepalived_vrrp[31554]: VRRP_Instance(VI_1) Sending/queueing gratuitous ARPs on eth0 for 192.168.8.20\n# ...\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1 and master2"',title:'"on',master1:!0,and:!0,'master2"':!0},"ip a s eth0\n# 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n#     link/ether 02:11:32:2b:87:ba brd ff:ff:ff:ff:ff:ff\n#     inet 192.168.8.21/24 brd 192.168.8.255 scope global noprefixroute eth0\n#        valid_lft forever preferred_lft forever\n#     inet 192.168.8.20/32 scope global eth0\n#        valid_lft forever preferred_lft forever\n")),(0,r.kt)("p",null,"If master 1 is down, the VIP will drift to master 2."),(0,r.kt)("h3",{id:"config-haproxy-on-masters"},"Config haproxy on Masters"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1 and master2"',title:'"on',master1:!0,and:!0,'master2"':!0},"yum install -y haproxy\n")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Config `/etc/haproxy/haproxy.cfg` referencing follows"),(0,r.kt)(o.Z,{language:"conf",mdxType:"CodeBlock"},"#---------------------------------------------------------------------\n# Example configuration for a possible web application.  See the\n# full configuration options online.\n#\n#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt\n#\n#---------------------------------------------------------------------\n\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n\n#---------------------------------------------------------------------\n# kubernetes apiserver frontend which proxys to the backends\n#---------------------------------------------------------------------\nfrontend kubernetes-apiserver\n    mode                    tcp\n    bind                    *:16443\n    option                  tcplog\n    default_backend         kubernetes-apiserver\n\n#---------------------------------------------------------------------\n# round robin balancing between the various backends\n#---------------------------------------------------------------------\nbackend kubernetes-apiserver\n    mode tcp\n    balance roundrobin\n    server kube-master1.k8s.io 192.168.8.21:6443 check\n    server kube-master2.k8s.io 192.168.8.22:6443 check\n\n#---------------------------------------------------------------------\n# collection haproxy statistics message\n#---------------------------------------------------------------------\nlisten stats\n    bind                    *:1080\n    stats auth              admin:awesomePassword\n    stats refresh           5s\n    stats realm             Haproxy\\ Statistics\n    stats uri               /admin?stats")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1 and master2"',title:'"on',master1:!0,and:!0,'master2"':!0},"systemctl start haproxy.service\nsystemctl enable haproxy.service\nsystemctl status haproxy.service\n# ...\n# Active: active (running) since\n# ...\n\nnetstat -lntup|grep haproxy\n# tcp        0      0 0.0.0.0:1080            0.0.0.0:*               LISTEN      19504/haproxy       \n# tcp        0      0 0.0.0.0:16443           0.0.0.0:*               LISTEN      19504/haproxy       \n# udp        0      0 0.0.0.0:55052           0.0.0.0:*                           19501/haproxy\n")),(0,r.kt)("h3",{id:"config-k8s-on-master-1"},"Config K8S on Master 1"),(0,r.kt)("div",{className:"admonition admonition-warning alert alert--danger"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"warning")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"The following commands must be performed on a master node that has VIP.\nHere resume the master node 1 is the VIP master."))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1"',title:'"on','master1"':!0},"mkdir /usr/local/kubernetes/manifests -p \n")),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Config `/usr/local/kubernetes/manifests/kubeadm-config.yaml` referencing follows"),(0,r.kt)(o.Z,{language:"yaml",mdxType:"CodeBlock"},"apiServer:\n  certSANs:\n  - kube-master1\n  - kube-master2\n  - kube-master.k8s.io\n  - 192.168.8.20\n  - 192.168.8.21\n  - 192.168.8.22\n  - 127.0.0.1\n  extraArgs:\n    authorization-mode: Node,RBAC\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta2\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrolPlaneEndpoint: kube-master1.k8s.io:16443\ncontrollerManager: {}\ndns:\n  type: CoreDNS\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: k8s.gcr.io\nkind: ClusterConfiguration\nkubernetesVersion: v1.18.0\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: 10.244.0.0/16\n  serviceSubnet: 10.1.0.0/16\nscheduler: {}")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1"',title:'"on','master1"':!0},"kubeadm init --config /usr/local/kubernetes/manifests/kubeadm-config.yaml\n# ...\n# mkdir -p $HOME/.kube\n# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n# sudo chown $(id -u):$(id -g) $HOME/.kube/config\n# ...\n# kubeadm join kube-master1.k8s.io:16443 --token cfkgse.2yzjhgn4ykn4ny8y \\\n#   --discovery-token-ca-cert-hash sha256:4a56b1dbc3456a1d11cf8ace3c22a8569702966ab035f666e2f96ba7fb5c9d39 \\\n#   --control-plane\n# ...\n# kubeadm join kube-master1.k8s.io:16443 --token cfkgse.2yzjhgn4ykn4ny8y \\\n#     --discovery-token-ca-cert-hash sha256:4a56b1dbc3456a1d11cf8ace3c22a8569702966ab035f666e2f96ba7fb5c9d39\n\nmkdir -p $HOME/.kube\ncp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nchown $(id -u):$(id -g) $HOME/.kube/config\nkubectl get nodes\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1"',title:'"on','master1"':!0},'kubectl get cs\n# NAME                 STATUS    MESSAGE             ERROR\n# scheduler            Healthy   ok                  \n# controller-manager   Healthy   ok                  \n# etcd-0               Healthy   {"health":"true"}\n\nkubectl get pods -n kube-system\n# NAME                                   READY   STATUS    RESTARTS   AGE\n# coredns-66bff467f8-259f6               1/1     Running   0          2m2s\n# coredns-66bff467f8-pj8ct               1/1     Running   0          2m3s\n# etcd-kube-master1                      1/1     Running   0          2m47s\n# kube-apiserver-kube-master1            1/1     Running   0          2m47s\n# kube-controller-manager-kube-master1   1/1     Running   1          3m8s\n# kube-proxy-lwxkb                       1/1     Running   0          2m3s\n# kube-scheduler-kube-master1            1/1     Running   0          2m46s\n')),(0,r.kt)("p",null,"Then you can config the flannel network referencing ",(0,r.kt)("strong",{parentName:"p"},"Install flannel plugin")," section."),(0,r.kt)("h3",{id:"config-k8s-on-master-2"},"Config K8S on Master 2"),(0,r.kt)("h4",{id:"copy-certification-files-from-master-1-to-2"},"Copy Certification Files from Master 1 to 2"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1"',title:'"on','master1"':!0},"ssh root@kube-master2 mkdir -p /etc/kubernetes/pki/etcd\nscp /etc/kubernetes/admin.conf root@kube-master2:/etc/kubernetes\nscp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*} root@kube-master2:/etc/kubernetes/pki\nscp /etc/kubernetes/pki/etcd/ca.* root@kube-master2:/etc/kubernetes/pki/etcd\n")),(0,r.kt)("h4",{id:"add-master-2-into-cluster"},"Add Master 2 into Cluster"),(0,r.kt)("p",null,"Run the '--control-plane' one which was output by the master 1."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master2"',title:'"on','master2"':!0},"kubeadm join kube-master1.k8s.io:16443 --token cfkgse.2yzjhgn4ykn4ny8y \\\n  --discovery-token-ca-cert-hash sha256:4a56b1dbc3456a1d11cf8ace3c22a8569702966ab035f666e2f96ba7fb5c9d39 \\\n  --control-plane\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master2"',title:'"on','master2"':!0},"kubectl get nodes\n# NAME           STATUS   ROLES    AGE    VERSION\n# kube-master1   Ready    master   23m    v1.18.0\n# kube-master2   Ready    master   110s   v1.18.0\n")),(0,r.kt)("h3",{id:"config-slave-nodes"},"Config Slave Nodes"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text",metastring:'title="on master2 etc/hosts"',title:'"on',master2:!0,'etc/hosts"':!0},"127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.8.20 kube-master.k8s.io kube-master\n192.168.8.21 kube-master1.k8s.io kube-master1\n192.168.8.22 kube-master2.k8s.io kube-master2\n192.168.8.31 kube-node1.k8s.io kube-node1\n192.168.8.32 kube-node2.k8s.io kube-node2\n192.168.8.33 kube-node3.k8s.io kube-node3\n")),(0,r.kt)("p",null,"Run the command WITH OUT '--control-plane' which was output by the master 1."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on slave nodes"',title:'"on',slave:!0,'nodes"':!0},"kubeadm join kube-master1.k8s.io:16443 --token cfkgse.2yzjhgn4ykn4ny8y \\\n    --discovery-token-ca-cert-hash sha256:4a56b1dbc3456a1d11cf8ace3c22a8569702966ab035f666e2f96ba7fb5c9d39\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl get nodes\n# NAME           STATUS   ROLES    AGE   VERSION\n# kube-master1   Ready    master   62m   v1.18.0\n# kube-master2   Ready    master   41m   v1.18.0\n# kube-node1     Ready    <none>   35m   v1.18.0\n# kube-node2     Ready    <none>   39s   v1.18.0\n\nkubectl get pods -n kube-system -o wide\n# NAME                                   READY   STATUS    RESTARTS   AGE     IP             NODE           NOMINATED NODE   READINESS GATES\n# coredns-66bff467f8-259f6               1/1     Running   0          62m     10.244.0.3     kube-master1   <none>           <none>\n# coredns-66bff467f8-pj8ct               1/1     Running   0          62m     10.244.0.2     kube-master1   <none>           <none>\n# etcd-kube-master1                      1/1     Running   0          63m     192.168.8.21   kube-master1   <none>           <none>\n# etcd-kube-master2                      1/1     Running   0          42m     192.168.8.22   kube-master2   <none>           <none>\n# kube-apiserver-kube-master1            1/1     Running   0          63m     192.168.8.21   kube-master1   <none>           <none>\n# kube-apiserver-kube-master2            1/1     Running   0          42m     192.168.8.22   kube-master2   <none>           <none>\n# kube-controller-manager-kube-master1   1/1     Running   2          63m     192.168.8.21   kube-master1   <none>           <none>\n# kube-controller-manager-kube-master2   1/1     Running   1          42m     192.168.8.22   kube-master2   <none>           <none>\n# kube-flannel-ds-dp56p                  1/1     Running   0          43m     192.168.8.22   kube-master2   <none>           <none>\n# kube-flannel-ds-j2l9t                  1/1     Running   0          53m     192.168.8.21   kube-master1   <none>           <none>\n# kube-flannel-ds-k4mvt                  1/1     Running   1          37m     192.168.8.31   kube-node1     <none>           <none>\n# kube-flannel-ds-z7nz8                  1/1     Running   1          2m33s   192.168.8.32   kube-node2     <none>           <none>\n# kube-proxy-5nsfj                       1/1     Running   0          2m33s   192.168.8.32   kube-node2     <none>           <none>\n# kube-proxy-8v5wz                       1/1     Running   0          37m     192.168.8.31   kube-node1     <none>           <none>\n# kube-proxy-lwxkb                       1/1     Running   0          62m     192.168.8.21   kube-master1   <none>           <none>\n# kube-proxy-wp984                       1/1     Running   0          43m     192.168.8.22   kube-master2   <none>           <none>\n# kube-scheduler-kube-master1            1/1     Running   2          63m     192.168.8.21   kube-master1   <none>           <none>\n# kube-scheduler-kube-master2            1/1     Running   2          42m     192.168.8.22   kube-master2   <none>           <none>\n\n")),(0,r.kt)("p",null,"The HA cluster is now ready to use."),(0,r.kt)("h3",{id:"test-the-ha-cluster"},"Test the HA Cluster"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master"',title:'"on','master"':!0},"kubectl create deployment nginx --image=nginx\n# deployment.apps/nginx created\n\nkubectl expose deployment nginx --port=80 --type=NodePort\n# service/nginx exposed\n\nkubectl get pod,svc\n# NAME                        READY   STATUS    RESTARTS   AGE\n# pod/nginx-f89759699-sm8zs   1/1     Running   0          43s\n\n# NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\n# service/kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP        67m\n# service/nginx        NodePort    10.1.167.111   <none>        80:30754/TCP   68s\n")),(0,r.kt)("p",null,"Then you can access the Nginx service with VIP:  ",(0,r.kt)("a",{parentName:"p",href:"http://192.168.8.20:30754"},"http://192.168.8.20:30754")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master1"',title:'"on','master1"':!0},"shutdown -h now\n# Connection to 192.168.8.21 closed by remote host.\n# Connection to 192.168.8.21 closed.\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:'title="on master2"',title:'"on','master2"':!0},"ip a s eth0\n# 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n#     link/ether 02:11:32:25:17:76 brd ff:ff:ff:ff:ff:ff\n#     inet 192.168.8.22/24 brd 192.168.8.255 scope global noprefixroute eth0\n#        valid_lft forever preferred_lft forever\n#     inet 192.168.8.20/32 scope global eth0\n#        valid_lft forever preferred_lft forever\n")),(0,r.kt)("p",null,"Then you can still access the Nginx service with VIP:  ",(0,r.kt)("a",{parentName:"p",href:"http://192.168.8.20:30754"},"http://192.168.8.20:30754")))}g.isMDXComponent=!0},11748:function(e,n,t){var a={"./locale":89234,"./locale.js":89234};function s(e){var n=r(e);return t(n)}function r(e){if(!t.o(a,e)){var n=new Error("Cannot find module '"+e+"'");throw n.code="MODULE_NOT_FOUND",n}return a[e]}s.keys=function(){return Object.keys(a)},s.resolve=r,e.exports=s,s.id=11748},26906:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-06-11-01-22-ae45c939ca7d9d1df37b160b2daa92ad.png"},53066:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-11-06-56-25-19c46d0e654369af7cdbbf80c0c23e70.png"},58803:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-11-06-57-01-057bb00a6dcf591d844e70e56e83a5d7.png"},95007:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-11-06-59-03-a796086b7ff01eb61e48f7fb970a14f0.png"},32711:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-11-07-01-15-6a73646300cfc7f9de4609d4f3ec3860.png"},382:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-11-07-02-01-d859a9d5737d9b66cc6e3a7330fc235e.png"},99194:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-11-07-03-11-eb0863756739848e9e67972806acdbb9.png"},81732:function(e,n,t){"use strict";n.Z=t.p+"assets/images/2022-06-11-07-04-39-53ef363df4d6a871bfadb2aa6bc283d6.png"}}]);